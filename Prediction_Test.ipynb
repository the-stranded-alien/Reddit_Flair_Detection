{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"final_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...tors=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input The URL for which Flair is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Reddit API using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = '4olKQ-Up2BCKWQ', client_secret = 'sZsEDlw6NzhL3TpD95p1QMeH81E', user_agent = 'Sahil Gupta', username = 'the_stranded_alien', password = 'strandedalien')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Data For This Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_features(pred_url):\n",
    "    \n",
    "    submission = reddit.submission(url=str(pred_url))\n",
    "\n",
    "\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_comments in submission.comments:\n",
    "        comment = comment + \" \" + top_comments.body\n",
    "\n",
    "    return submission.title, comment, pred_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanURL(url):\n",
    "    url = url.replace(\"/\",\" \").replace(\".\",\" \").replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    url = re.sub(r\"[0-9]+\", \"\", url)\n",
    "    new_url = []\n",
    "    for u in (url.split()):\n",
    "        # print(u)\n",
    "        if u != \"reddit\" and u != \"comments\" and u != \"india\" and u != \"https:\" and u != \"http:\" and len(u) > 4 and u != \"www\" and u != \"com\":\n",
    "            new_url.append(u)\n",
    "    return \" \".join(new_url)\n",
    "\n",
    "def cleaner(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^0-9a-z #+_]\",\"\",sentence)\n",
    "    sentence = re.sub(\"[/(){}\\[\\]\\|@,;]\", \" \", sentence)\n",
    "    sentence = ' '.join(word for word in sentence.split() if word not in stop_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_feature(title, comments, url):\n",
    "    u = cleanURL(url)\n",
    "    t = cleaner(title)\n",
    "    c = cleaner(comments)\n",
    "    \n",
    "    return str(u) + str(t) + str(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_prediction():\n",
    "    pred_url = input()\n",
    "    t,c,u = fetch_features(pred_url)\n",
    "    feature = cleaned_feature(t,c,u)\n",
    "    feature = np.array(feature).reshape((1,))\n",
    "    prediction = model.predict(feature)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit = reddit.subreddit('india')\n",
    "# for submission in subreddit.top(limit=10):\n",
    "#     print(\"ID : \" + submission.id + \" -> URL : \" + submission.permalink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/india/comments/d0urih/ill_wait/\n"
     ]
    }
   ],
   "source": [
    "x = make_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Science/Technology']\n"
     ]
    }
   ],
   "source": [
    "print(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
